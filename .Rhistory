plot(density(train$LPR),main=LPR)
plot(density(train$PEG),main=PEG)
# -------------------------
# MATRIZ DE SCATTERPLOTS
# -------------------------
# DATASET: study_habits
#------
# Fazendo mais uma matriz de scatterplots,
# incluindo a correla??o no painel superior mas agora com proporcionalidade.
openPNG('08_featscattcorr')
panel.corprop <- function(x, y, ...)
{
par(usr = c(0, 1, 0, 1))
txt <- as.character(format(cor(x, y), digits=2))
text(0.5, 0.5, txt, cex = 6* abs(cor(x, y)))
}
pairs(train[1:5], main="Study Habits Dataset With Correlations", pch=21, bg=c("red","green3","blue")[unclass(train$UNS)], upper.panel=panel.corprop)
# Closes the graphics file
dev.off()
# Bloco: Business Intelligence e Big Data Analytics: Valor
# Disciplina: Big Data Analytics com R
# Analysis Phase
#clear environment
rm(list=ls())
#clear Console = ctrl+l
# -------------------------
# install necessary packages
# -------------------------
# install.packages("RColorBrewer", dependencies=TRUE)
# install.packages("ggplot2", dependencies=TRUE)
# install.packages("scatterplot3d", dependencies=TRUE)
# install.packages("lattice", dependencies=TRUE)
# install.packages("party", dependencies=TRUE)
# install.packages("tm", dependencies=TRUE)
# install.packages("SnowballC", dependencies=TRUE)
# install.packages("wordcloud", dependencies=TRUE)
# install.packages("corrplot", dependencies=TRUE)
# install.packages("deldir", dependencies=TRUE)
# install.packages(c("sp", "maps", "maptools", "mapproj"), dependencies=TRUE)
# install.packages("reshape", dependencies=TRUE)
# loading packages
library(dplyr) # bind rows
require('RColorBrewer') # color paletes for graphs
library(reshape)
library(ggplot2)
# SET THE HOME DIR FOR WINDOWS OR LINUX ENVIRONMENTS
homeDir='C:/Users/Carlos/Documents/MEGAsync/MIT/Modulo_A_Valor/Atual/infnet_moduloA/'
#homeDir='/home/carlos/MEGAsync/MIT/infnet_moduloA/'
setwd(homeDir)
# loading common functions
library(stringr) # string manipulation
source('functions/fn_load_data.R') # calculate prediction metrics
source('functions/fn_check_missing_data.R') # check for missing data
# Common function to plot graphics - in the future create a sourced funtion for reusability
openPNG <- function(imagename){
png(filename = paste(getwd(),'/graphs/',imagename,'.png', sep = ''),
width = 670, height = 379)
}
# -------------------------
# MAIN
# -------------------------
# load data sets
load_data()
# check for missing data
check_missing_data(full)
# -------------------------
# BOXPLOTS
# -------------------------
# DATASET: Study
#study_habits <- read.csv("train_data.csv")
#study_habits_test <- read.csv("test_data.csv")
#head(descr)
STG <- subset(descr, Attribute == "STG", select=c(Description),1)
SCG <- subset(descr, Attribute == "SCG", select=c(Description),1)
STR <- subset(descr, Attribute == "STR", select=c(Description),1)
LPR <- subset(descr, Attribute == "LPR", select=c(Description),1)
PEG <- subset(descr, Attribute == "PEG", select=c(Description),1)
UNS <- subset(descr, Attribute == "UNS", select=c(Description),1)
# create a unique dataframe with the count of distinct values for each column
unique <- apply(full, 2, function(x)length(unique(x)))
uniqueDf <- cbind(read.table(text = names(unique)), unique)
# Opens the graphics file
openPNG('01_all_features_barplot')
# Basic Barplot
my_bar=barplot(uniqueDf$unique , border=F , names.arg=uniqueDf$V1 , las=2 , col=brewer.pal(6,"Accent") , ylim=c(0,125) , main="" )
abline(v=c(6.1) , col="grey")
# Add the text
text(my_bar, uniqueDf$unique+4.04 , paste("n = ",uniqueDf$unique,sep="") ,cex=1)
# Closes the graphics file
dev.off()
# example of melt function
fullPivot <- melt(full[1:5])
summary(full$STG)
summary(full$SCG)
summary(full$STR)
summary(full$LPR)
summary(full$PEG)
summary(full$UNS)
# plot boxplots
# Opens the graphics file
openPNG('02_full_pivot_boxplots')
ggplot(fullPivot, aes(x=variable, y=value, fill=variable)) +
geom_boxplot(alpha=0.4) +
stat_summary(fun.y=mean, geom="point", shape=20, size=10, color="red", fill="red") +
theme(legend.position="none") +
scale_fill_brewer(palette="Set3")
# Closes the graphics file
dev.off()
# Boxplots for each dataset independent variable
openPNG('03_STG_boxplot')
boxplot(STG~UNS, data=train, main=STG
,horizontal=TRUE # Horizontal boxplot
,las = 1 # Make all labels horizontal
,col=brewer.pal(5,"Accent")
,notch = TRUE # nOTCHES FOR CI for median
,boxwex = 0.5 # Width of box as proportion of original
,whisklty = 1 # Whisker line type; 1 = solid line
,staplelty =0 # Staple - line at the end; 0 - none
)
# Closes the graphics file
dev.off()
# Boxplots for each dataset independent variable
openPNG('04_SCG_boxplot')
boxplot(SCG~UNS, data=train, main=SCG
,horizontal=TRUE # Horizontal boxplot
,las = 1 # Make all labels horizontal
,col=brewer.pal(5,"Accent")
,notch = FALSE # Notches FOR CI for median
,boxwex = 0.5 # Width of box as proportion of original
,whisklty = 1 # Whisker line type; 1 = solid line
,staplelty =0 # Staple - line at the end; 0 - none
)
# Closes the graphics file
dev.off()
# Boxplots for each dataset independent variable
openPNG('05_STR_boxplot')
boxplot(STR~UNS, data=train, main=STR
,horizontal=TRUE # Horizontal boxplot
,las = 1 # Make all labels horizontal
,col=brewer.pal(5,"Accent")
,notch = TRUE # nOTCHES FOR CI for median
,boxwex = 0.5 # Width of box as proportion of original
,whisklty = 1 # Whisker line type; 1 = solid line
,staplelty =0 # Staple - line at the end; 0 - none
)
# Closes the graphics file
dev.off()
# Boxplots for each dataset independent variable
openPNG('06_LPR_boxplot')
boxplot(LPR~UNS, data=train, main=LPR
,horizontal=TRUE # Horizontal boxplot
,las = 1 # Make all labels horizontal
,col=brewer.pal(5,"Accent")
,notch = TRUE # nOTCHES FOR CI for median
,boxwex = 0.5 # Width of box as proportion of original
,whisklty = 1 # Whisker line type; 1 = solid line
,staplelty =0 # Staple - line at the end; 0 - none
)
# Closes the graphics file
dev.off()
# Boxplots for each dataset independent variable
openPNG('07_PEG_boxplot')
boxplot(PEG~UNS, data=train, main=PEG
,horizontal=TRUE # Horizontal boxplot
,las = 1 # Make all labels horizontal
,col=brewer.pal(5,"Accent")
,notch = FALSE # nOTCHES FOR CI for median
,boxwex = 0.5 # Width of box as proportion of original
,whisklty = 1 # Whisker line type; 1 = solid line
,staplelty =0 # Staple - line at the end; 0 - none
)
# Closes the graphics file
dev.off()
# -------------------------
# HISTOGRAMAS
# -------------------------
# DATASET: study habits
# Olhando os histogramas como densidades
plot(density(train$STG),main=STG)
plot(density(train$SCG),main=SCG)
plot(density(train$STR),main=STR)
plot(density(train$LPR),main=LPR)
plot(density(train$PEG),main=PEG)
# -------------------------
# MATRIZ DE SCATTERPLOTS
# -------------------------
# DATASET: study_habits
#------
# Fazendo mais uma matriz de scatterplots,
# incluindo a correla??o no painel superior mas agora com proporcionalidade.
openPNG('08_featscattcorr')
panel.corprop <- function(x, y, ...)
{
par(usr = c(0, 1, 0, 1))
txt <- as.character(format(cor(x, y), digits=2))
text(0.5, 0.5, txt, cex = 6* abs(cor(x, y)))
}
pairs(train[1:5], main="Study Habits Dataset With Correlations", pch=21, bg=c("red","green3","blue")[unclass(train$UNS)], upper.panel=panel.corprop)
# Closes the graphics file
dev.off()
# Bloco: Business Intelligence e Big Data Analytics: Valor
# Disciplina: Big Data Analytics com R
# Prediction Phase
#clear environment
rm(list=ls())
#clear Console = ctrl+l
# -------------------------
# necessary packages
# -------------------------
# install.packages("party", dependencies=TRUE)
# install.packages("rpart", dependencies=TRUE)
# install.packages("rpart.plot", dependencies=TRUE)
# install.packages("rattle", dependencies=TRUE)
# install.packages("RWeka", dependencies=TRUE)
# install.packages("ipred", dependencies=TRUE)
# install.packages("randomForest", dependencies=TRUE)
# install.packages("C50", dependencies=TRUE)
# install.packages("RColorBrewer", dependencies=TRUE)
# loading packages
library(rpart) # decision trees
library(rpart.plot) # decision trees visualization
library(rattle)	# Visualization
library(party) # decision trees
library(dplyr) # bind rows
library(RWeka) # Algoritmos J48 e PART.
library(ipred) # Bagging.
library(randomForest) # Random Forest.
library(C50) # Algoritmo C5.0.
library(stringr) # string manipulation
require('RColorBrewer') # color paletes for graphs
homeDir='C:/Users/Carlos/Documents/MEGAsync/MIT/Modulo_A_Valor/Atual/infnet_moduloA/'
#homeDir='/home/carlos/MEGAsync/MIT/infnet_moduloA/'
setwd(homeDir)
# loading common functions
source('functions/fn_load_data.R') # calculate prediction metrics
source('functions/fn_prediction_metrics.R') # calculate prediction metrics
source('functions/fn_model_randomForest.R') # train and run model randomForest
# functions used only in this script
openPNG <- function(imagename){
png(filename = paste(getwd(),'/graphs/',imagename,'.png', sep = ''),
width = 670, height = 379)
}
# Bloco: Business Intelligence e Big Data Analytics: Valor
# Disciplina: Big Data Analytics com R
# Prediction Phase
#clear environment
rm(list=ls())
#clear Console = ctrl+l
# -------------------------
# necessary packages
# -------------------------
# install.packages("party", dependencies=TRUE)
# install.packages("rpart", dependencies=TRUE)
# install.packages("rpart.plot", dependencies=TRUE)
# install.packages("rattle", dependencies=TRUE)
# install.packages("RWeka", dependencies=TRUE)
# install.packages("ipred", dependencies=TRUE)
# install.packages("randomForest", dependencies=TRUE)
# install.packages("C50", dependencies=TRUE)
# install.packages("RColorBrewer", dependencies=TRUE)
# loading packages
library(rpart) # decision trees
library(rpart.plot) # decision trees visualization
library(rattle)	# Visualization
library(party) # decision trees
library(dplyr) # bind rows
library(RWeka) # Algoritmos J48 e PART.
library(ipred) # Bagging.
library(randomForest) # Random Forest.
library(C50) # Algoritmo C5.0.
library(stringr) # string manipulation
library(stringr) # string manipulation
require('RColorBrewer') # color paletes for graphs
# SET THE HOME DIR FOR WINDOWS OR LINUX ENVIRONMENTS
homeDir='C:/Users/Carlos/Documents/MEGAsync/MIT/Modulo_A_Valor/Atual/infnet_moduloA/'
#homeDir='/home/carlos/MEGAsync/MIT/infnet_moduloA/'
setwd(homeDir)
# loading common functions
source('functions/fn_load_data.R') # calculate prediction metrics
source('functions/fn_prediction_metrics.R') # calculate prediction metrics
source('functions/fn_model_randomForest.R') # train and run model randomForest
# functions used only in this script
openPNG <- function(imagename){
png(filename = paste(getwd(),'/graphs/',imagename,'.png', sep = ''),
width = 670, height = 379)
}
model_ctree <- function(train, test, formula) {
# -------------------------
# Decision trees
# -------------------------
# Train the model
# Conditional Inference Trees {party}
fit_ctree <- ctree(formula, data=train)
# Confusion????? matrix to check the model's prediction capacity.
#table(predict(fit_ctree), train$UNS)
# Printing the model's rule and showing the tree
# what is the p inside the tree node
#print(fit_ctree)
#plot(fit_ctree)
# Anothe tree - simpler
#plot(fit_ctree, type="simple")
# Testando o modelo no conjunto de testes
predict(fit_ctree, newdata = test)
#table(testPred, test$UNS)
}
model_rpart <- function(train, test, formula) {
# -------------------------
# Train a model using rpart.
# Recursive Partitioning and Regression Trees
# -------------------------
fit_rpart <- rpart(formula, data=train)
# Resumo.
#summary(fit_rpart)
# Make predictions
# The parameter "type='class'" is used for classification trees
#predictions <- predict(fit_rpart, train[,1:5], type="class")
# Confusion??? matrix for the model
#table(predictions, train$UNS)
# Make predictions
# The parameter "type='class'" is used for classification trees
predict(fit_rpart, test, type="class")
# Confusion??? matrix for the predictions
#table(predictions, test$UNS)
# Printing the rules
#print(fit_rpart)
# Graph with 'rpart.plot'.
#prp(fit_rpart)
# Graph with 'rattle'.
#fancyRpartPlot(fit_rpart)
}
model_J48 <- function(train, test, formula) {
# -------------------------
# ALGORITMO C4.5
# Cria a árvore de forma a maximixar o ganho de informação (information gain)
# (diferença de entropia). Chamado de J48 no Weka.
# Entropia: quantidade necessária de informação para identificar a classe de um caso
# Ganho de informação: é a redução esperada da entropia ao utilizarmos um atributo na árvore.
# Bom post explicativo (inglês): http://stackoverflow.com/questions/1859554/what-is-entropy-and-information-gain
# -------------------------
# Criando o modelo.
fit_J48 <- J48(formula, data=train)
# Resumo.
#summary(fit_J48)
# Fazendo predições.
predict(fit_J48, test)
# Matriz de confusão das predições.
#table(predictions, test$UNS)
# Gráfico da árvore.
#plot(fit_J48)
}
model_PART <- function(train, test, formula) {
# -------------------------
# PART
# Sistema de regras que cria árvores de decisão C4.5 podadas e extrai regras, retirando
# então os dados podados do conjunto de treinamento. O processo é repetido até que
# todas as instâncias sejam cobertas pelas regras extraídas.
#
# Sem visualização com gráfico!
# -------------------------
# Criando o modelo.
fit_PART <- PART(formula, data=train)
# Resumo.
#summary(fit_PART)
# Fazendo predições.
predict(fit_PART, test)
# Matriz de confusão das predições.
#table(predictions, test$UNS)
#plot(fit_PART)
}
model_bagging <- function(train, test, formula) {
# -------------------------
# BAGGING CART
# Bootstrapped Aggregation (Bagging) é um método ensemble, ou seja,
# cria múltiplos modelos do mesmo tipo a partir de amostras diferentes
# do conjunto completo de dados. Os resultados de cada modelo separado
# são combinados para oferecer um resultado melhor.
# Sem visualização com gráfico!
# -------------------------
# Criando o modelo.
fit_bagging <- bagging(formula, data=train)
# Resumo.
#summary(fit_bagging)
# Fazendo predições.
predict(fit_bagging, test, type="class")
# Matriz de confusão das predições.
#table(predictions4, test$UNS)
}
model_C5.0 <- function(train, test, formula) {
# -------------------------
# ALGORITMO C5.0
# Evolução do algoritmo C4.5 que teve seu código fonte liberado recentemente.
# -------------------------
# Criando o modelo.
# p parâmetro 'trials' indica o número de iterações desejado.
fit_C5.0 <- C5.0(formula, data=train, trials=10)
# Resumo.
#print(fit_C5.0)
# Fazendo predições.
predict(fit_C5.0, test)
# Gráfico da árvore.
#plot(fit_C5.0)
}
# -------------------------
# MAIN
# -------------------------
# load data sets
load_data()
# variables to store results
i <- 0
model_names <- c()
model_accuracies <- c()
# Setting seed for run
set.seed(12345)
# -------------------------
# Decision trees
# -------------------------
# Formula for ctree
# Specifying UNS as the dependent variable and the others are undependent
#myFormulactree <- UNS ~ STG + SCG + STR + LPR + PEG
# Formula for dependent variable
# Specifying UNS as the dependent variable and the others are undependent
myFormula <- UNS~.
# -------------------------
# Decision trees
# -------------------------
#run model - Calculate and print metrics
i <- i + 1
model_names[i] <- 'ctree'
model_accuracies[i] <- prediction_metrics(model_ctree(train, testx, myFormula), testy)
# -------------------------
# Train a model using rpart.
# Recursive Partitioning and Regression Trees
# -------------------------
#run model - Calculate and print metrics
i <- i + 1
model_names[i] <- 'rpart'
model_accuracies[i] <- prediction_metrics(model_rpart(train, testx, myFormula), testy)
# -------------------------
# ALGORITMO C4.5
# Cria a árvore de forma a maximixar o ganho de informação (information gain)
# -------------------------
#run model - Calculate and print metrics
i <- i + 1
model_names[i] <- 'J48'
model_accuracies[i] <- prediction_metrics(model_J48(train, testx, myFormula), testy)
# -------------------------
# PART
# Sistema de regras que cria árvores de decisão C4.5 podadas e extrai regras
# -------------------------
#run model - Calculate and print metrics
i <- i + 1
model_names[i] <- 'PART'
model_accuracies[i] <- prediction_metrics(model_PART(train, testx, myFormula), testy)
# -------------------------
# BAGGING CART
# Bootstrapped Aggregation (Bagging)
# -------------------------
#run model - Calculate and print metrics
i <- i + 1
model_names[i] <- 'bagging'
model_accuracies[i] <- prediction_metrics(model_bagging(train, testx, myFormula), testy)
# -------------------------
# RANDOM FOREST
# Variação do Bagging de árvores de decisao
# -------------------------
#run model - Calculate and print metrics
i <- i + 1
model_names[i] <- 'randomForest'
model_accuracies[i] <- prediction_metrics(model_randomForest(train, testx, myFormula), testy)
# -------------------------
# ALGORITMO C5.0
# Evolução do algoritmo C4.5 que teve seu código fonte liberado recentemente.
# -------------------------
#run model - Calculate and print metrics
i <- i + 1
model_names[i] <- 'C5.0'
model_accuracies[i] <- prediction_metrics(model_C5.0(train, testx, myFormula), testy)
result <- data.frame(model_names, model_accuracies)
result
fit_randomForest <- randomForest(myFormula, data=train, importance=TRUE)
yhat <- predict(fit_randomForest, testx)
ydiff <- as.character(yhat)==as.character(testy)
print(fit_randomForest)
print(fit_randomForest$importance)
yy <- data.frame(testx, testy, yhat, ydiff)
#yy
randomForestImportance <- fit_randomForest$importance
MeanDecreaseAccuracy <- randomForestImportance[1:5, 5]
MeanDecreaseGini <- randomForestImportance[1:5, 6]
# Opens the graphics file
openPNG('09_barplot_MeanDecreaseAccuracy')
barplot(MeanDecreaseAccuracy[order(MeanDecreaseAccuracy, decreasing = TRUE)],
col = brewer.pal(5,'Accent'),
main = 'Mean Decreased Accuracy')
dev.off()
# Opens the graphics file
openPNG('10_barplot_MeanDecreaseGini')
barplot(MeanDecreaseGini[order(MeanDecreaseGini, decreasing = TRUE)],
col = brewer.pal(5,'Dark2'),
main = 'Mean Decreased Gini')
dev.off()
# Setting seed for run
set.seed(12345)
myFormula <- UNS~PEG+LPR
prediction_metrics(model_randomForest(train, testx, myFormula), testy)
set.seed(12345)
fit_randomForest <- randomForest(myFormula, data=train, importance=TRUE)
set.seed(12345)
yhat <- predict(fit_randomForest, testx)
ydiff <- as.character(yhat)==as.character(testy)
print(fit_randomForest)
print(fit_randomForest$importance)
